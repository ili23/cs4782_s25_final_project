{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installations and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Mount your Google Drive; this allows the runtime environment to access your drive.\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "ipython = IPython.get_ipython()\n",
    "ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "ipython.run_line_magic(\"autoreload\", \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11afe06d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import lightning as L\n",
    "from lightning.pytorch import loggers as pl_loggers\n",
    "\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.architecture.assembled import AssembledModel\n",
    "from models.train_loop import PatchTSTTrainer\n",
    "from data.data_loader import TSDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_tst = AssembledModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeffreyxiang/Desktop/HW/cs4782_s25_final_project/data/data_loader.py:30: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)\n",
      "/Users/jeffreyxiang/Desktop/HW/cs4782_s25_final_project/data/data_loader.py:31: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)\n",
      "/Users/jeffreyxiang/Desktop/HW/cs4782_s25_final_project/data/data_loader.py:32: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)\n",
      "/Users/jeffreyxiang/Desktop/HW/cs4782_s25_final_project/data/data_loader.py:33: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_stamp['year'] = df_stamp.date.apply(lambda row: row.year, 1)\n",
      "/Users/jeffreyxiang/Desktop/HW/cs4782_s25_final_project/data/data_loader.py:34: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_stamp['dayofweek'] = df_stamp.date.apply(\n",
      "/Users/jeffreyxiang/Desktop/HW/cs4782_s25_final_project/data/data_loader.py:30: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)\n",
      "/Users/jeffreyxiang/Desktop/HW/cs4782_s25_final_project/data/data_loader.py:31: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)\n",
      "/Users/jeffreyxiang/Desktop/HW/cs4782_s25_final_project/data/data_loader.py:32: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)\n",
      "/Users/jeffreyxiang/Desktop/HW/cs4782_s25_final_project/data/data_loader.py:33: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_stamp['year'] = df_stamp.date.apply(lambda row: row.year, 1)\n",
      "/Users/jeffreyxiang/Desktop/HW/cs4782_s25_final_project/data/data_loader.py:34: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df_stamp['dayofweek'] = df_stamp.date.apply(\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"./models/results\"\n",
    "logs_output_dir = \"./models/results/logs\"\n",
    "model_trainer = PatchTSTTrainer(patch_tst, output_dir)\n",
    "device = \"cpu\"\n",
    "dataloader = TSDataLoader(\"./data/data_files/electricity/electricity.csv\", device=device)\n",
    "train_dataloader, val_dataloader = dataloader.get_data_loaders()\n",
    "\n",
    "tb_logger = pl_loggers.TensorBoardLogger(save_dir=logs_output_dir)\n",
    "\n",
    "trainer = L.Trainer(max_epochs=20, \n",
    "                    check_val_every_n_epoch=2,\n",
    "                    num_sanity_val_steps=0, \n",
    "                    logger=tb_logger\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type           | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | model | AssembledModel | 42.6 M | train\n",
      "1 | loss  | MSELoss        | 0      | train\n",
      "-------------------------------------------------\n",
      "42.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "42.6 M    Total params\n",
      "170.290   Total estimated model params size (MB)\n",
      "168       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 0:   0%|          | 0/656 [03:14<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/656 [00:00<?, ?it/s] x shape after normalization: torch.Size([32, 24, 321])\n",
      "torch.Size([32, 1, 321])\n",
      "(tensor([[[-1.4461,  0.9718, -0.8735,  ...,  0.5439, -1.7983, -0.0617]],\n",
      "\n",
      "        [[-0.6018, -0.0219, -0.0350,  ..., -0.0488, -1.1463, -0.5188]],\n",
      "\n",
      "        [[-0.6062,  1.9849,  0.0464,  ..., -0.5073,  0.8756,  0.6403]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 4.3197, -1.4264, -0.6736,  ..., -0.2458, -1.4822, -1.5186]],\n",
      "\n",
      "        [[-1.6080, -0.5859, -1.0111,  ..., -3.6641, -2.3621, -1.8269]],\n",
      "\n",
      "        [[ 0.7003, -0.3149,  6.1392,  ...,  0.0887, -0.6270,  0.3663]]]), tensor([[[-1.6970,  0.3312, -1.1862,  ..., -0.7497, -1.9525, -0.6519]],\n",
      "\n",
      "        [[-0.9959, -0.0381, -0.4581,  ..., -0.7611, -1.4107, -0.7451]],\n",
      "\n",
      "        [[-0.8927,  2.1477, -0.2609,  ..., -0.3429, -0.1801,  0.4732]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1247, -0.9890, -0.8188,  ..., -1.9729, -1.8319, -1.4525]],\n",
      "\n",
      "        [[-1.4348, -0.7885, -1.0859,  ..., -2.9304, -2.0421, -1.6107]],\n",
      "\n",
      "        [[ 3.2787, -0.2499,  6.8084,  ...,  0.4342, -0.3124,  0.6035]]]), tensor([[[-1.8310,  0.2520, -1.3377,  ..., -0.2650, -2.2398, -0.7331]],\n",
      "\n",
      "        [[-1.3405,  0.5559, -0.8186,  ..., -1.1697, -1.8117, -1.1284]],\n",
      "\n",
      "        [[-1.4080,  0.6598, -0.8254,  ...,  0.0753, -0.7653,  1.1236]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.5627, -0.5926, -0.8932,  ..., -1.7150, -1.9603, -1.8153]],\n",
      "\n",
      "        [[-1.3225, -0.6801, -0.9770,  ..., -2.5575, -1.7536, -1.4338]],\n",
      "\n",
      "        [[ 3.9777, -0.1590,  7.6661,  ...,  0.2877, -0.2540,  0.5564]]]), tensor([[[-1.8572,  0.1247, -1.3784,  ..., -0.4491, -2.3487, -0.6362]],\n",
      "\n",
      "        [[-1.5317,  0.6933, -1.0152,  ..., -1.3910, -2.0576, -1.2991]],\n",
      "\n",
      "        [[-1.6860,  0.3278, -1.1318,  ..., -1.5242, -0.8878,  0.1083]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.3168, -1.2059, -0.8731,  ..., -0.5726, -1.2339, -1.3604]],\n",
      "\n",
      "        [[-1.2526, -0.5112, -0.8900,  ..., -2.8043, -1.6493, -1.3582]],\n",
      "\n",
      "        [[ 4.8073,  0.0143,  9.0333,  ..., -0.7945, -0.1141,  0.7330]]]), tensor([[[-1.6445,  0.1413, -1.2041,  ..., -0.6608, -2.0820, -0.0512]],\n",
      "\n",
      "        [[-1.5914,  0.2663, -1.0802,  ..., -1.4641, -2.1834, -1.1322]],\n",
      "\n",
      "        [[-1.9018, -0.1213, -1.3834,  ..., -2.1037, -1.0009,  0.8922]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.5687, -1.1210, -0.3741,  ...,  0.2908, -0.1393, -0.9006]],\n",
      "\n",
      "        [[-1.0938, -0.5672, -0.7671,  ..., -2.9523, -1.5456, -0.9946]],\n",
      "\n",
      "        [[ 5.0154,  0.1710,  9.4478,  ..., -0.7287,  0.0583,  0.5166]]]), tensor([[[-1.6281, -0.3641, -1.1885,  ..., -0.3391, -2.0719, -0.0152]],\n",
      "\n",
      "        [[-1.3506,  0.1406, -0.8740,  ..., -0.9061, -1.9143, -0.5107]],\n",
      "\n",
      "        [[-1.9101, -0.2389, -1.4593,  ...,  0.0333, -0.4183,  1.4711]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.9686, -0.4722,  0.2871,  ...,  0.9716,  0.7059,  0.0942]],\n",
      "\n",
      "        [[-0.8590, -0.5811, -0.4345,  ..., -2.6504, -1.2928, -0.5680]],\n",
      "\n",
      "        [[ 4.8856,  0.3115,  9.4425,  ..., -0.9404, -0.0933,  0.3401]]]), tensor([[[-1.6758, -0.8120, -1.2327,  ..., -0.2154, -2.0467,  0.0511]],\n",
      "\n",
      "        [[-1.2180, -0.1666, -0.7582,  ..., -0.7532, -1.7202,  0.1219]],\n",
      "\n",
      "        [[-1.8382, -0.5958, -1.3321,  ...,  0.1247, -0.7721,  1.6526]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.9013, -0.2542,  0.8665,  ...,  1.3402,  1.3022,  0.4201]],\n",
      "\n",
      "        [[-0.3474, -0.1576,  0.0223,  ..., -2.1722, -0.8122, -0.1195]],\n",
      "\n",
      "        [[ 2.2161, -0.2894,  8.5128,  ..., -0.1266, -0.7309, -0.1583]]]), tensor([[[-1.4280, -1.0824, -1.1987,  ..., -0.4542, -1.9772,  0.0456]],\n",
      "\n",
      "        [[-1.1545, -0.4645, -0.7030,  ..., -0.8763, -1.6920, -0.1875]],\n",
      "\n",
      "        [[-1.8004, -0.5143, -1.3101,  ...,  0.6775, -1.0419,  1.3105]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.8570, -0.3324,  1.2059,  ...,  1.3911,  1.6120,  0.7304]],\n",
      "\n",
      "        [[ 0.0386,  0.0379,  0.4346,  ..., -1.9047, -0.5111, -0.1532]],\n",
      "\n",
      "        [[ 1.8661, -0.5539,  7.5106,  ..., -0.0361, -1.0626, -0.4728]]]), tensor([[[-1.5999, -0.8127, -1.2149,  ..., -0.7635, -2.0699,  0.2278]],\n",
      "\n",
      "        [[-1.0966, -2.4411, -0.6633,  ..., -1.0056, -1.5768, -0.7524]],\n",
      "\n",
      "        [[-1.6676, -1.1194, -1.2870,  ...,  0.7878, -1.0169,  1.6130]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.8820, -0.3152,  1.4076,  ...,  1.8018,  1.6718,  0.7542]],\n",
      "\n",
      "        [[ 0.3653,  0.0320,  1.0071,  ..., -1.7691, -0.3581, -0.0339]],\n",
      "\n",
      "        [[ 2.6956, -0.3584,  7.1616,  ..., -0.7127, -1.2632, -1.0456]]]), tensor([[[-1.6954, -0.2486, -1.2565,  ..., -0.7238, -2.1386,  0.6223]],\n",
      "\n",
      "        [[-1.1069, -3.1470, -0.6732,  ..., -0.9599, -1.5913, -0.3929]],\n",
      "\n",
      "        [[-1.6456, -1.1453, -1.3111,  ...,  0.3369, -0.4075,  1.9085]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.0359, -0.2625,  1.5533,  ...,  1.4735,  1.8591,  0.5104]],\n",
      "\n",
      "        [[ 0.5484,  0.1785,  1.1238,  ..., -2.0160, -0.1854, -0.1916]],\n",
      "\n",
      "        [[ 2.1340,  0.3638,  3.4470,  ..., -1.1037, -1.7615, -1.5222]]]), tensor([[[-1.8130,  0.8502, -1.3518,  ..., -0.4129, -2.2522,  0.4666]],\n",
      "\n",
      "        [[-1.1480, -2.9978, -0.7316,  ..., -0.9611, -1.6978, -0.6646]],\n",
      "\n",
      "        [[-1.7867, -0.4621, -1.3671,  ...,  1.1865,  1.0001,  2.1515]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.1976, -0.3091,  1.6291,  ...,  1.7070,  1.8618,  0.5192]],\n",
      "\n",
      "        [[ 0.6766,  0.0484,  1.2693,  ..., -2.6295, -0.1980, -0.1915]],\n",
      "\n",
      "        [[ 1.9259,  1.2275, -0.6908,  ..., -1.2967, -2.1500, -1.9480]]]), tensor([[[-1.8355,  1.0365, -1.3276,  ..., -0.5483, -2.1604,  0.1034]],\n",
      "\n",
      "        [[-1.3220, -2.5925, -0.8970,  ..., -0.8223, -1.9676, -0.7898]],\n",
      "\n",
      "        [[-1.9833,  0.4104, -1.4859,  ...,  1.2279,  0.3811,  1.8309]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.9478, -0.2574,  1.4578,  ...,  1.5255,  0.7093, -0.0673]],\n",
      "\n",
      "        [[ 0.9054,  0.2344,  1.3979,  ..., -2.8384, -0.1044,  0.2572]],\n",
      "\n",
      "        [[ 2.8070,  0.6274, -0.8312,  ..., -1.6546, -2.2966, -2.1532]]]), tensor([[[-1.6789,  0.2248, -1.3050,  ..., -0.6858, -2.0648,  0.3252]],\n",
      "\n",
      "        [[-1.5191,  0.5101, -1.2052,  ..., -1.4178, -2.4038, -1.3725]],\n",
      "\n",
      "        [[-2.1070,  1.2587, -1.6166,  ...,  1.3382, -0.1767,  1.2870]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.2461, -0.2288,  0.9805,  ...,  0.8633, -0.0817, -0.4823]],\n",
      "\n",
      "        [[ 0.7055,  0.0676,  1.3984,  ..., -3.2638, -0.2532,  0.0889]],\n",
      "\n",
      "        [[ 1.8136,  0.5067, -0.8301,  ..., -1.7034, -2.3939, -1.8699]]]), tensor([[[-1.3913, -0.6958, -0.9689,  ..., -0.1962, -1.8121,  0.4334]],\n",
      "\n",
      "        [[-1.3807, -0.4336, -1.0542,  ..., -1.4981, -2.1870, -1.3554]],\n",
      "\n",
      "        [[-2.0987,  1.4265, -1.5919,  ...,  0.5598,  0.1575,  1.7946]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5981,  2.1832,  0.3442,  ..., -1.2711,  0.2822, -0.8427]],\n",
      "\n",
      "        [[ 0.2897, -0.1612,  0.9134,  ..., -3.6648, -0.6889, -0.6012]],\n",
      "\n",
      "        [[ 2.1621,  1.5714, -0.6453,  ..., -0.9499, -2.2617, -1.5769]]]), tensor([[[-0.8052,  0.1148, -0.3583,  ...,  0.1495, -1.3717,  0.9744]],\n",
      "\n",
      "        [[-0.9738, -0.5385, -0.5273,  ...,  0.1072, -1.0077, -0.9571]],\n",
      "\n",
      "        [[-1.7816,  1.0011, -1.4809,  ...,  0.2627, -0.0956,  1.9231]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.8298,  1.9437,  0.1176,  ..., -0.6446,  0.0553, -1.0228]],\n",
      "\n",
      "        [[-0.1956, -0.5410,  0.3616,  ..., -3.3824, -1.1820, -1.0488]],\n",
      "\n",
      "        [[ 2.8865,  0.3794, -0.4776,  ..., -0.9979, -1.9953, -1.3863]]]), tensor([[[-0.2545,  1.0171,  0.1859,  ...,  0.5692, -0.9383,  1.3781]],\n",
      "\n",
      "        [[-0.1639,  0.0656,  0.2332,  ...,  1.3597, -0.4962, -0.1858]],\n",
      "\n",
      "        [[-1.3452, -0.3717, -1.0088,  ...,  0.6884,  0.3750,  2.3093]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.7630, -0.2440, -0.4442,  ..., -1.5661, -0.2583, -0.7728]],\n",
      "\n",
      "        [[-0.0518, -0.1289,  0.2538,  ..., -3.5059, -1.2766, -1.6931]],\n",
      "\n",
      "        [[-0.4474, -0.2895, -0.4086,  ..., -1.0104, -1.8651, -1.2186]]]), tensor([[[ 0.0049,  0.0039,  0.5212,  ...,  0.9969, -0.6444,  1.6745]],\n",
      "\n",
      "        [[ 0.2888,  0.6997,  0.8216,  ...,  1.1454, -0.0716,  0.2496]],\n",
      "\n",
      "        [[-0.6076,  0.6219, -0.2744,  ...,  0.8694,  1.3292,  3.2534]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0904, -0.9746, -0.7465,  ..., -1.8987, -0.3290, -0.1781]],\n",
      "\n",
      "        [[-0.5715,  0.5474, -0.2804,  ..., -3.8560, -1.7856, -2.0136]],\n",
      "\n",
      "        [[-0.6010, -0.5321, -0.3851,  ..., -1.0901, -1.7601, -1.0150]]]), tensor([[[ 0.2156, -0.1792,  0.6950,  ...,  0.8311, -0.6263,  1.4092]],\n",
      "\n",
      "        [[ 0.6704,  0.2303,  1.0424,  ...,  0.8361,  0.1467,  0.5279]],\n",
      "\n",
      "        [[-0.0934,  0.8881,  0.2621,  ...,  1.2154,  2.0511,  3.1075]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5960, -1.0102, -0.8469,  ..., -1.9338, -0.1735, -0.1082]],\n",
      "\n",
      "        [[-0.8577,  0.5838, -0.5776,  ..., -3.9330, -1.9952, -2.2055]],\n",
      "\n",
      "        [[ 1.9391, -0.4109, -0.4277,  ..., -0.4765, -1.7917, -1.1203]]]), tensor([[[ 3.6723e-01,  1.2685e-01,  9.4789e-01,  ..., -3.5690e-01,\n",
      "          -4.5056e-01,  1.4466e+00]],\n",
      "\n",
      "        [[ 7.0184e-01,  4.9580e-02,  1.2372e+00,  ...,  9.1691e-01,\n",
      "          -4.2973e-01,  4.7365e-01]],\n",
      "\n",
      "        [[ 1.9593e-01, -6.1994e-02,  5.1186e-01,  ...,  1.5034e+00,\n",
      "           2.1582e+00,  2.9093e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 5.8733e-01, -1.0632e+00, -8.8692e-01,  ..., -2.0049e-01,\n",
      "           5.7962e-01, -3.7012e-04]],\n",
      "\n",
      "        [[-1.0113e+00,  5.5029e-01, -7.5438e-01,  ..., -3.8947e+00,\n",
      "          -2.0455e+00, -1.9116e+00]],\n",
      "\n",
      "        [[ 2.0730e+00, -7.3607e-01, -8.8043e-01,  ..., -6.3580e-01,\n",
      "          -2.3811e+00, -1.5223e+00]]]), tensor([[[ 0.3976,  0.0372,  0.9790,  ..., -0.3718, -0.3276,  1.0346]],\n",
      "\n",
      "        [[ 0.7852,  0.0864,  1.3025,  ...,  0.8562, -0.4756,  0.5354]],\n",
      "\n",
      "        [[ 0.3761, -0.2200,  0.7175,  ...,  1.5289,  2.1455,  2.8762]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.2266, -0.9474, -0.7111,  ...,  0.3663,  0.5034, -0.1346]],\n",
      "\n",
      "        [[-0.9876,  0.5966, -0.7966,  ..., -3.6797, -1.9511, -1.6796]],\n",
      "\n",
      "        [[ 1.8028, -0.9087, -1.0335,  ..., -0.9756, -2.7150, -1.7445]]]), tensor([[[ 0.3190, -2.6330,  0.9036,  ..., -1.2927, -0.4749,  0.6507]],\n",
      "\n",
      "        [[ 0.8234,  0.1320,  1.3351,  ...,  1.2179, -0.2981,  0.5150]],\n",
      "\n",
      "        [[ 0.4802, -0.0840,  0.8978,  ...,  1.3263,  1.8741,  2.7325]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.7492, -1.0970, -0.6865,  ...,  0.6456,  0.2207, -0.5969]],\n",
      "\n",
      "        [[-0.9889,  0.6010, -0.7190,  ..., -4.0313, -2.0898, -1.9917]],\n",
      "\n",
      "        [[ 1.7575, -0.8154, -0.9726,  ..., -1.3120, -2.4271, -1.8291]]]), tensor([[[ 0.0145, -3.1894,  0.5070,  ..., -1.7878, -0.7724,  0.1912]],\n",
      "\n",
      "        [[ 0.7120,  0.2220,  1.1471,  ...,  0.7430, -0.5602,  0.3711]],\n",
      "\n",
      "        [[ 0.5410,  0.0169,  0.9712,  ...,  1.5131,  1.5591,  2.3111]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.3027, -1.2130, -0.6424,  ..., -0.3655, -0.0230, -0.5369]],\n",
      "\n",
      "        [[-0.9341,  0.5540, -0.6450,  ..., -4.1114, -2.1400, -1.8987]],\n",
      "\n",
      "        [[ 1.8782, -1.0249, -0.7805,  ..., -1.0004, -2.0634, -1.5664]]]), tensor([[[-0.4504, -2.5838,  0.0508,  ..., -0.5615, -0.9171, -0.2942]],\n",
      "\n",
      "        [[ 0.1457,  1.3078,  0.6114,  ...,  0.2268, -0.8809, -0.0206]],\n",
      "\n",
      "        [[ 0.5611,  0.2441,  0.9494,  ...,  1.7579,  1.7337,  1.8765]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.2094, -1.5172, -0.5359,  ...,  0.4323,  0.8059, -0.6025]],\n",
      "\n",
      "        [[-0.8970,  0.1747, -0.6299,  ..., -4.2510, -2.1030, -2.0481]],\n",
      "\n",
      "        [[ 0.1985, -0.8240, -0.3737,  ..., -0.8668, -1.5824, -1.1980]]]), tensor([[[-0.7235, -1.6902, -0.1452,  ..., -0.0599, -1.2029, -0.9464]],\n",
      "\n",
      "        [[-0.3396,  2.8182,  0.0652,  ..., -1.2704,  0.6604,  0.0462]],\n",
      "\n",
      "        [[ 0.0871,  1.1833,  0.5511,  ...,  0.6360,  1.5638,  1.0583]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.5316, -1.2977, -0.7228,  ...,  1.6832,  1.0030, -0.8916]],\n",
      "\n",
      "        [[-1.1100, -0.3149, -0.6498,  ..., -4.1859, -2.1321, -2.0272]],\n",
      "\n",
      "        [[ 0.3441, -0.4596,  0.1707,  ..., -0.0794, -1.0873, -0.6079]]]))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_trainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining completed.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/4782/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:561\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/4782/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:48\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.strategy.launcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     51\u001b[39m     _call_teardown_hook(trainer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/4782/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:599\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    592\u001b[39m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    593\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    594\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    595\u001b[39m     ckpt_path,\n\u001b[32m    596\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    597\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    598\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n\u001b[32m    602\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/4782/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1012\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1007\u001b[39m \u001b[38;5;28mself\u001b[39m._signal_connector.register_signal_handlers()\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1017\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: trainer tearing down\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/4782/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1056\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1054\u001b[39m         \u001b[38;5;28mself\u001b[39m._run_sanity_check()\n\u001b[32m   1055\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m-> \u001b[39m\u001b[32m1056\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1057\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1058\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.state\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/4782/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:216\u001b[39m, in \u001b[36m_FitLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    215\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_start()\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_end()\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/4782/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:455\u001b[39m, in \u001b[36m_FitLoop.advance\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.trainer.profiler.profile(\u001b[33m\"\u001b[39m\u001b[33mrun_training_epoch\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepoch_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/4782/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:150\u001b[39m, in \u001b[36m_TrainingEpochLoop.run\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done:\n\u001b[32m    149\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m         \u001b[38;5;28mself\u001b[39m.on_advance_end(data_fetcher)\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/4782/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:320\u001b[39m, in \u001b[36m_TrainingEpochLoop.advance\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33m\"\u001b[39m\u001b[33mrun_training_batch\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.lightning_module.automatic_optimization:\n\u001b[32m    319\u001b[39m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m         batch_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mautomatic_optimization\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    321\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    322\u001b[39m         batch_output = \u001b[38;5;28mself\u001b[39m.manual_optimization.run(kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/4782/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:192\u001b[39m, in \u001b[36m_AutomaticOptimization.run\u001b[39m\u001b[34m(self, optimizer, batch_idx, kwargs)\u001b[39m\n\u001b[32m    185\u001b[39m         closure()\n\u001b[32m    187\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m result = closure.consume_result()\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/4782/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:270\u001b[39m, in \u001b[36m_AutomaticOptimization._optimizer_step\u001b[39m\u001b[34m(self, batch_idx, train_step_and_backward_closure)\u001b[39m\n\u001b[32m    267\u001b[39m     \u001b[38;5;28mself\u001b[39m.optim_progress.optimizer.step.increment_ready()\n\u001b[32m    269\u001b[39m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moptimizer_step\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[32m    280\u001b[39m     \u001b[38;5;28mself\u001b[39m.optim_progress.optimizer.step.increment_completed()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/4782/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:176\u001b[39m, in \u001b[36m_call_lightning_module_hook\u001b[39m\u001b[34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m pl_module._current_fx_name = hook_name\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m    179\u001b[39m pl_module._current_fx_name = prev_fx_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/4782/lib/python3.11/site-packages/lightning/pytorch/core/module.py:1302\u001b[39m, in \u001b[36mLightningModule.optimizer_step\u001b[39m\u001b[34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[39m\n\u001b[32m   1271\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimizer_step\u001b[39m(\n\u001b[32m   1272\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1273\u001b[39m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1276\u001b[39m     optimizer_closure: Optional[Callable[[], Any]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1277\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1278\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[32m   1279\u001b[39m \u001b[33;03m    the optimizer.\u001b[39;00m\n\u001b[32m   1280\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1300\u001b[39m \n\u001b[32m   1301\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1302\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/4782/lib/python3.11/site-packages/lightning/pytorch/core/optimizer.py:154\u001b[39m, in \u001b[36mLightningOptimizer.step\u001b[39m\u001b[34m(self, closure, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[33m\"\u001b[39m\u001b[33mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_strategy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[38;5;28mself\u001b[39m._on_after_step()\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/4782/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py:239\u001b[39m, in \u001b[36mStrategy.optimizer_step\u001b[39m\u001b[34m(self, optimizer, closure, model, **kwargs)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl.LightningModule)\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprecision_plugin\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/4782/lib/python3.11/site-packages/lightning/pytorch/plugins/precision/precision.py:123\u001b[39m, in \u001b[36mPrecision.optimizer_step\u001b[39m\u001b[34m(self, optimizer, model, closure, **kwargs)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[32m    122\u001b[39m closure = partial(\u001b[38;5;28mself\u001b[39m._wrap_closure, model, optimizer, closure)\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/4782/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:75\u001b[39m, in \u001b[36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     73\u001b[39m instance._step_count += \u001b[32m1\u001b[39m\n\u001b[32m     74\u001b[39m wrapped = func.\u001b[34m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/4782/lib/python3.11/site-packages/torch/optim/optimizer.py:385\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    380\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    381\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    382\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    383\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    388\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/4782/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     74\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m'\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     75\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     78\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/4782/lib/python3.11/site-packages/torch/optim/adamw.py:164\u001b[39m, in \u001b[36mAdamW.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    163\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.enable_grad():\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m         loss = \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.param_groups:\n\u001b[32m    167\u001b[39m     params_with_grad = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/4782/lib/python3.11/site-packages/lightning/pytorch/plugins/precision/precision.py:109\u001b[39m, in \u001b[36mPrecision._wrap_closure\u001b[39m\u001b[34m(self, model, optimizer, closure)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrap_closure\u001b[39m(\n\u001b[32m     97\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     98\u001b[39m     model: \u001b[33m\"\u001b[39m\u001b[33mpl.LightningModule\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     99\u001b[39m     optimizer: Steppable,\n\u001b[32m    100\u001b[39m     closure: Callable[[], Any],\n\u001b[32m    101\u001b[39m ) -> Any:\n\u001b[32m    102\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[33;03m    hook is called.\u001b[39;00m\n\u001b[32m    104\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    107\u001b[39m \n\u001b[32m    108\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     closure_result = \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m     \u001b[38;5;28mself\u001b[39m._after_closure(model, optimizer)\n\u001b[32m    111\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m closure_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/4782/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:146\u001b[39m, in \u001b[36mClosure.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> Optional[Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     \u001b[38;5;28mself\u001b[39m._result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result.loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/4782/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/4782/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:131\u001b[39m, in \u001b[36mClosure.closure\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;129m@torch\u001b[39m.enable_grad()\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> ClosureResult:\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m step_output.closure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    134\u001b[39m         \u001b[38;5;28mself\u001b[39m.warning_cache.warn(\u001b[33m\"\u001b[39m\u001b[33m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/4782/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:319\u001b[39m, in \u001b[36m_AutomaticOptimization._training_step\u001b[39m\u001b[34m(self, kwargs)\u001b[39m\n\u001b[32m    308\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Performs the actual train step with the tied hooks.\u001b[39;00m\n\u001b[32m    309\u001b[39m \n\u001b[32m    310\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    315\u001b[39m \n\u001b[32m    316\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    317\u001b[39m trainer = \u001b[38;5;28mself\u001b[39m.trainer\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m training_step_output = \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtraining_step\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer.strategy.post_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m training_step_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m trainer.world_size > \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/4782/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:328\u001b[39m, in \u001b[36m_call_strategy_hook\u001b[39m\u001b[34m(trainer, hook_name, *args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.strategy.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m    331\u001b[39m pl_module._current_fx_name = prev_fx_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/4782/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py:391\u001b[39m, in \u001b[36mStrategy.training_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model != \u001b[38;5;28mself\u001b[39m.lightning_module:\n\u001b[32m    390\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_redirection(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.lightning_module, \u001b[33m\"\u001b[39m\u001b[33mtraining_step\u001b[39m\u001b[33m\"\u001b[39m, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m391\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlightning_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/HW/cs4782_s25_final_project/models/train_loop.py:21\u001b[39m, in \u001b[36mPatchTSTTrainer.training_step\u001b[39m\u001b[34m(self, batch, batch_idx)\u001b[39m\n\u001b[32m     19\u001b[39m     \u001b[38;5;28mself\u001b[39m.train_loss = \u001b[32m0\u001b[39m\n\u001b[32m     20\u001b[39m x, y = batch[\u001b[32m0\u001b[39m], batch[\u001b[32m1\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m y_hat = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.loss(y_hat, y)\n\u001b[32m     23\u001b[39m \u001b[38;5;28mself\u001b[39m.train_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/4782/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/4782/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/HW/cs4782_s25_final_project/models/architecture/assembled.py:34\u001b[39m, in \u001b[36mAssembledModel.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mx shape after normalization:\u001b[39m\u001b[33m\"\u001b[39m, x.shape)\n\u001b[32m     33\u001b[39m x = \u001b[38;5;28mself\u001b[39m.variable_splitter(x)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mx shape after variable splitting:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m)\n\u001b[32m     35\u001b[39m patches = \u001b[38;5;28mself\u001b[39m.patcher(x)\n\u001b[32m     36\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mpatches shape after patching:\u001b[39m\u001b[33m\"\u001b[39m, patches.shape)\n",
      "\u001b[31mAttributeError\u001b[39m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "trainer.fit(model_trainer, train_dataloader, val_dataloader)\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "4782",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
